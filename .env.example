# Trading Research Assistant - Environment Variables Example
# Copy this file to .env and fill in your values
# Notes:
# - This file is designed for multi-provider LLM routing with strict fallbacks.
# - Llama3 (Ollama local) is the last-resort fallback for all tasks.

## =================================================================
## Application
## =================================================================

APP_ENV=development
APP_TIMEZONE=Asia/Yerevan # write what you need

## =================================================================
## Candles: Market Data Providers
## =================================================================

# OANDA API (Forex candles)
OANDA_API_KEY=
OANDA_ACCOUNT_ID=
OANDA_BASE_URL=https://api-fxpractice.oanda.com

# Twelve Data API (optional fallback candles)
TWELVE_DATA_API_KEY=
TWELVE_DATA_BASE_URL=https://api.twelvedata.com

## =================================================================
## News: Providers
## =================================================================

# GDELT API (no key required)
GDELT_BASE_URL=https://api.gdeltproject.org

# NewsAPI (requires key)
NEWSAPI_API_KEY=
NEWSAPI_BASE_URL=https://newsapi.org

## =================================================================
## Storage
## =================================================================

STORAGE_SQLITE_DB_PATH=db/forex_research_assistant.sqlite3
STORAGE_ARTIFACTS_DIR=artifacts
STORAGE_MIGRATION_PATH=src/storage/sqlite/migrations

## =================================================================
## Runtime
## =================================================================

RUNTIME_MVP_SYMBOLS_RAW=EURUSD,GBPUSD,USDJPY
RUNTIME_MVP_TIMEFRAME=1m
RUNTIME_MVP_EXPIRY_SECONDS=60

# If false, the app should skip LLM calls and keep the pipeline safe.
RUNTIME_LLM_ENABLED=true

# If LLM is enabled, these control refresh cadence.
RUNTIME_LLM_CALL_INTERVAL_SECONDS=300
RUNTIME_NEWS_REFRESH_INTERVAL_SECONDS=300
RUNTIME_MARKET_DATA_WINDOW_CANDLES=300

## =================================================================
## LLM / AI CONFIGURATION
## =================================================================

# Router behavior
# - sequential: try primary, then fallbacks in order
# - strict: fail if primary unavailable (not recommended for research workflow)
LLM_ROUTER_MODE=sequential

# Verification stage (Role D)
LLM_VERIFIER_ENABLED=true

# Global LLM defaults
LLM_MAX_RETRIES=3
LLM_TIMEOUT_SECONDS=60.0
LLM_TEMPERATURE=0.2

## -----------------------------------------------------------------
## Providers: Connection Strings and Keys
## -----------------------------------------------------------------

# Ollama local (Windows laptop)
OLLAMA_LOCAL_URL=http://localhost:11434

# Ollama server (Runpod or your Linux server)
# Example: http://123.45.67.89:11434
OLLAMA_SERVER_URL=http://your-server-ip:11434

# --- DeepSeek (OpenAI-compatible API style) ---
DEEPSEEK_API_KEY=
DEEPSEEK_API_BASE=https://api.deepseek.com

# --- ChatGPT (Openai) ---
OPENAI_API_KEY=
OPENAI_API_BASE=https://api.openai.com/v1
OPENAI_ORG_ID=

# --- Gemini (Google) ---
GOOGLE_API_KEY=
GOOGLE_API_BASE=https://generativelanguage.googleapis.com/v1

# --- Perplexity ---
PERPLEXITY_API_KEY=
PERPLEXITY_API_BASE=https://api.perplexity.ai

## -----------------------------------------------------------------
## Task routing
## -----------------------------------------------------------------
# Rules:
# - Provider is one of: ollama_local, ollama_server, deepseek_api
# - Model is the model name as that provider expects.
# - Llama3 local is the last fallback for every task.

## TASK: TECH ANALYSIS (Role A)
# Goal: Interpret indicators and features, identify regimes and invalidation points.

TECH_PRIMARY_PROVIDER=deepseek_api
TECH_PRIMARY_MODEL=deepseek-chat

TECH_FALLBACK1_PROVIDER=ollama_server
TECH_FALLBACK1_MODEL=deepseek-r1:32b

TECH_FALLBACK2_PROVIDER=ollama_server
TECH_FALLBACK2_MODEL=qwen2.5:32b

TECH_FALLBACK3_PROVIDER=ollama_local
TECH_FALLBACK3_MODEL=llama3:latest

## TASK: NEWS ANALYSIS (Role B)
# Goal: Summarize news, relevance scoring, risk bullets, no invented facts.

NEWS_PRIMARY_PROVIDER=ollama_local
NEWS_PRIMARY_MODEL=qwen2.5:7b

NEWS_FALLBACK1_PROVIDER=ollama_server
NEWS_FALLBACK1_MODEL=qwen2.5:32b

NEWS_FALLBACK2_PROVIDER=ollama_server
NEWS_FALLBACK2_MODEL=fino1-8b

NEWS_FALLBACK3_PROVIDER=ollama_local
NEWS_FALLBACK3_MODEL=llama3:latest

## TASK: SYNTHESIS (Role C)
# Goal: Final brief, risks, confidence guidance, strict research-only output.

SYNTHESIS_PRIMARY_PROVIDER=ollama_server
SYNTHESIS_PRIMARY_MODEL=llama3.1:70b

SYNTHESIS_FALLBACK1_PROVIDER=deepseek_api
SYNTHESIS_FALLBACK1_MODEL=deepseek-chat

SYNTHESIS_FALLBACK2_PROVIDER=ollama_local
SYNTHESIS_FALLBACK2_MODEL=llama3:latest

## TASK: VERIFICATION (Role D)
# Goal: Detect unsupported claims, policy compliance, consistency checks.

VERIFIER_PRIMARY_PROVIDER=ollama_local
VERIFIER_PRIMARY_MODEL=phi3.5:latest

VERIFIER_FALLBACK1_PROVIDER=ollama_server
VERIFIER_FALLBACK1_MODEL=granite3-8b:latest

VERIFIER_FALLBACK2_PROVIDER=deepseek_api
VERIFIER_FALLBACK2_MODEL=deepseek-chat

VERIFIER_FALLBACK3_PROVIDER=ollama_local
VERIFIER_FALLBACK3_MODEL=llama3:latest

## -----------------------------------------------------------------
## Local hardware gating (used by scripts/python/check_gpu.py and runtime heuristics)
## -----------------------------------------------------------------
# If VRAM is below this threshold, prefer small local models and avoid heavy local pulls.
LOCAL_GPU_MIN_VRAM_GB=8.0

# If true, allow CPU-only inference fallbacks for local models when GPU is limited.
LOCAL_ALLOW_CPU_FALLBACK=true

## =================================================================
## Hugging Face / Transformers Caches (for scripts/python/download_models.py)
## =================================================================
# All caches are redirected into the project folder so Windows user cache is not polluted.
# Recommended: keep this under models/ so it can be managed and cleaned easily.

# Root folder for all model-related caches and downloads managed by this project
MODEL_STORAGE_DIR=models

# Hugging Face home (token, settings, general HF state)
HF_HOME=models/.cache/huggingface

# Hugging Face Hub cache (model snapshots, refs, blobs)
HUGGINGFACE_HUB_CACHE=models/.cache/huggingface/hub

# Transformers cache (legacy but still used by some codepaths)
TRANSFORMERS_CACHE=models/.cache/huggingface/transformers

# Datasets cache (if later you use datasets for eval or corpora)
HF_DATASETS_CACHE=models/.cache/huggingface/datasets

# Optional: cache for tokenizers (sometimes used separately)
HF_TOKENIZERS_CACHE=models/.cache/huggingface/tokenizers

# Optional: cache for custom modules loaded from HF repos
HF_MODULES_CACHE=models/.cache/huggingface/modules

# Optional: where to store GGUF files if you download them directly
GGUF_CACHE_DIR=models/gguf

# Optional: where to store compiled/optimized artifacts
MODEL_BUILD_CACHE_DIR=models/.cache/build

# Disable progress bars in non-interactive environments
HF_HUB_DISABLE_PROGRESS_BARS=1
TQDM_DISABLE=1

# Hugging Face token (optional unless you pull gated models or need higher rate limits)
HF_TOKEN=
HUGGINGFACE_HUB_TOKEN=

## =================================================================
## Logging
## =================================================================
LOG_LEVEL=INFO
LOG_CONSOLE_LEVEL=WARNING
LOG_SPLIT_FILES=true
LOG_ENABLE_HTTP_FILE=false
LOG_MASK_AUTH=true
LOG_RETENTION=90 days
LOG_COMPRESSION=zip