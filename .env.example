# Trading Research Assistant - Environment Variables Example
# Copy this file to .env and fill in your values
# Notes:
# - This file is designed for multi-provider LLM routing with strict fallbacks.
# - Llama3 (Ollama local) is the last-resort fallback for all tasks.

## =================================================================
## Application
## =================================================================

APP_ENV=development
APP_TIMEZONE=Asia/Yerevan # write what you need

## =================================================================
## Candles: Market Data Providers
## =================================================================

# OANDA API (Forex candles)
OANDA_API_KEY=
OANDA_ACCOUNT_ID=
OANDA_BASE_URL=https://api-fxpractice.oanda.com

# Twelve Data API (optional fallback candles)
TWELVE_DATA_API_KEY=
TWELVE_DATA_BASE_URL=https://api.twelvedata.com

## =================================================================
## News: Providers
## =================================================================

# GDELT API (no key required)
GDELT_BASE_URL=https://api.gdeltproject.org

# NewsAPI (requires key)
NEWSAPI_API_KEY=
NEWSAPI_BASE_URL=https://newsapi.org

## =================================================================
## Storage
## =================================================================

STORAGE_SQLITE_DB_PATH=db/forex_research_assistant.sqlite3
STORAGE_ARTIFACTS_DIR=artifacts
STORAGE_MIGRATION_PATH=src/storage/sqlite/migrations

## =================================================================
## Runtime
## =================================================================

RUNTIME_MVP_SYMBOLS_RAW=EURUSD,GBPUSD,USDJPY
RUNTIME_MVP_TIMEFRAME=1m
RUNTIME_MVP_EXPIRY_SECONDS=60

# If false, the app should skip LLM calls and keep the pipeline safe.
RUNTIME_LLM_ENABLED=true

# If LLM is enabled, these control refresh cadence.
RUNTIME_LLM_CALL_INTERVAL_SECONDS=300
RUNTIME_NEWS_REFRESH_INTERVAL_SECONDS=300
RUNTIME_MARKET_DATA_WINDOW_CANDLES=300

# Environment selector
# local: local Ollama
# server: remote Ollama
RUNTIME_ENV=local

## =================================================================
## LLM / AI CONFIGURATION
## =================================================================

# Router behavior
# - sequential: try primary, then fallbacks in order
# - strict: use only primary provider/model, no fallback or last resort (one attempt only)
LLM_ROUTER_MODE=sequential

# Verification stage (Role D)
LLM_VERIFIER_ENABLED=true
LLM_VERIFIER_MODE=soft
LLM_VERIFIER_MAX_REPAIRS=1

# Global LLM defaults
LLM_MAX_RETRIES=3
LLM_TIMEOUT_SECONDS=60.0
LLM_TEMPERATURE=0.2

# Per-Task Timeouts (optional, overrides LLM_TIMEOUT_SECONDS)
TECH_TIMEOUT_SECONDS=90 
NEWS_TIMEOUT_SECONDS=240
SYNTHESIS_TIMEOUT_SECONDS=240
VERIFIER_TIMEOUT_SECONDS=500

# Per-Task Temperature (optional, overrides LLM_TEMPERATURE)
TECH_TEMPERATURE=
NEWS_TEMPERATURE=
SYNTHESIS_TEMPERATURE=
VERIFIER_TEMPERATURE=

# Per-Provider Timeouts (optional, overrides task timeouts)
OLLAMA_LOCAL_TIMEOUT_SECONDS=240
OLLAMA_SERVER_TIMEOUT_SECONDS=180
DEEPSEEK_API_TIMEOUT_SECONDS=180

# Per-Provider Per-Task Timeouts for Ollama Local (defaults: news/synthesis/verifier=240s)
OLLAMA_LOCAL_TECH_TIMEOUT_SECONDS=120
OLLAMA_LOCAL_NEWS_TIMEOUT_SECONDS=240.0
OLLAMA_LOCAL_SYNTHESIS_TIMEOUT_SECONDS=240.0
OLLAMA_LOCAL_VERIFIER_TIMEOUT_SECONDS=240.0

## -----------------------------------------------------------------
## Providers: Connection Strings and Keys
## -----------------------------------------------------------------

# Ollama local (Windows laptop)
OLLAMA_LOCAL_URL=http://localhost:11434

# Ollama server (Runpod or your Linux server)
# IMPORTANT: Replace "your-server-ip" with actual server IP/domain (e.g., http://123.45.67.89:11434)
# If placeholder is not replaced, URL will be ignored by normalization (must be valid http:// or https:// URL)
OLLAMA_SERVER_URL=http://your-server-ip:11434

# --- DeepSeek (OpenAI-compatible API style) ---
DEEPSEEK_API_KEY=
DEEPSEEK_API_BASE=https://api.deepseek.com

# --- FUTURE STUB: OpenAI (not implemented in current version) ---
OPENAI_API_KEY=
OPENAI_API_BASE=https://api.openai.com/v1
OPENAI_ORG_ID=

# --- FUTURE STUB: Gemini (Google) (not implemented in current version) ---
GOOGLE_API_KEY=
GOOGLE_API_BASE=https://generativelanguage.googleapis.com/v1

# --- FUTURE STUB: Perplexity (not implemented in current version) ---
PERPLEXITY_API_KEY=
PERPLEXITY_API_BASE=https://api.perplexity.ai

# -----------------------------------------------------------------
## Task routing (environment branches)
# -----------------------------------------------------------------
# Concept:
# - We do NOT treat "ollama_server" as a fallback for local environment.
# - The router selects a branch first: local or server.
# - Inside the selected branch we try providers in order.
# - Global last resort for every task: ollama_local / llama3:latest
#
# Rules:
# - Provider is one of: ollama_local, ollama_server, deepseek_api
# - Model is the model name as that provider expects.

# Global last resort (always)
LLM_LAST_RESORT_PROVIDER=ollama_local
LLM_LAST_RESORT_MODEL=llama3:latest

## =================================================================
## TASK: TECH ANALYSIS (Role A)
## Goal: Interpret indicators and features, identify regimes and invalidation points.
## =================================================================

# Local branch
TECH_LOCAL_PRIMARY_PROVIDER=deepseek_api
TECH_LOCAL_PRIMARY_MODEL=deepseek-chat

TECH_LOCAL_FALLBACK1_PROVIDER=ollama_local
TECH_LOCAL_FALLBACK1_MODEL=llama3:latest

# Server branch
TECH_SERVER_PRIMARY_PROVIDER=ollama_server
TECH_SERVER_PRIMARY_MODEL=deepseek-r1:32b

TECH_SERVER_FALLBACK1_PROVIDER=ollama_server
TECH_SERVER_FALLBACK1_MODEL=qwen2.5:32b

TECH_SERVER_FALLBACK2_PROVIDER=deepseek_api
TECH_SERVER_FALLBACK2_MODEL=deepseek-chat

## =================================================================
## TASK: NEWS ANALYSIS (Role B)
## Goal: Summarize news, relevance scoring, risk bullets, no invented facts.
## =================================================================

# Local branch
NEWS_LOCAL_PRIMARY_PROVIDER=ollama_local
NEWS_LOCAL_PRIMARY_MODEL=qwen2.5:7b

NEWS_LOCAL_FALLBACK1_PROVIDER=ollama_local
NEWS_LOCAL_FALLBACK1_MODEL=llama3:latest

# Server branch
NEWS_SERVER_PRIMARY_PROVIDER=ollama_server
NEWS_SERVER_PRIMARY_MODEL=qwen2.5:32b

NEWS_SERVER_FALLBACK1_PROVIDER=ollama_server
NEWS_SERVER_FALLBACK1_MODEL=fino1-8b

NEWS_SERVER_FALLBACK2_PROVIDER=ollama_local
NEWS_SERVER_FALLBACK2_MODEL=llama3:latest

## =================================================================
## TASK: SYNTHESIS (Role C)
## Goal: Final brief, risks, confidence guidance, strict research-only output.
## =================================================================

# Local branch
SYNTHESIS_LOCAL_PRIMARY_PROVIDER=ollama_local
SYNTHESIS_LOCAL_PRIMARY_MODEL=llama3:latest

SYNTHESIS_LOCAL_FALLBACK1_PROVIDER=deepseek_api
SYNTHESIS_LOCAL_FALLBACK1_MODEL=deepseek-chat

# Server branch
SYNTHESIS_SERVER_PRIMARY_PROVIDER=ollama_server
SYNTHESIS_SERVER_PRIMARY_MODEL=llama3.1:70b

SYNTHESIS_SERVER_FALLBACK1_PROVIDER=deepseek_api
SYNTHESIS_SERVER_FALLBACK1_MODEL=deepseek-chat

SYNTHESIS_SERVER_FALLBACK2_PROVIDER=ollama_local
SYNTHESIS_SERVER_FALLBACK2_MODEL=llama3:latest

## =================================================================
## TASK: VERIFICATION (Role D)
## Goal: Detect unsupported claims, policy compliance, consistency checks.
## =================================================================

# Local branch
VERIFIER_LOCAL_PRIMARY_PROVIDER=ollama_local
VERIFIER_LOCAL_PRIMARY_MODEL=phi3.5:latest

VERIFIER_LOCAL_FALLBACK1_PROVIDER=deepseek_api
VERIFIER_LOCAL_FALLBACK1_MODEL=deepseek-chat

VERIFIER_LOCAL_FALLBACK2_PROVIDER=ollama_local
VERIFIER_LOCAL_FALLBACK2_MODEL=llama3:latest

# Server branch
VERIFIER_SERVER_PRIMARY_PROVIDER=ollama_server
VERIFIER_SERVER_PRIMARY_MODEL=granite3.3:8b

VERIFIER_SERVER_FALLBACK1_PROVIDER=deepseek_api
VERIFIER_SERVER_FALLBACK1_MODEL=deepseek-chat

VERIFIER_SERVER_FALLBACK2_PROVIDER=ollama_local
VERIFIER_SERVER_FALLBACK2_MODEL=llama3:latest

## -----------------------------------------------------------------
## Preflight (used by runtime/preflight.py)
## -----------------------------------------------------------------
# Timeout for model download during preflight (0 = no timeout)
PREFLIGHT_DOWNLOAD_TIMEOUT_SECONDS=0

## -----------------------------------------------------------------
## Local hardware gating (used by scripts/python/check_gpu.py and runtime heuristics)
## -----------------------------------------------------------------
# If VRAM is below this threshold, prefer small local models and avoid heavy local pulls.
LOCAL_GPU_MIN_VRAM_GB=8.0

# If true, allow CPU-only inference fallbacks for local models when GPU is limited.
LOCAL_ALLOW_CPU_FALLBACK=true

## =================================================================
## Hugging Face / Transformers Caches (for scripts/python/download_models.py)
## =================================================================
# All caches are redirected into the project folder so Windows user cache is not polluted.
# Recommended: keep this under models/ so it can be managed and cleaned easily.

# Root folder for all model-related caches and downloads managed by this project
MODEL_STORAGE_DIR=models

# Hugging Face home (token, settings, general HF state)
HF_HOME=models/.cache/huggingface

# Hugging Face Hub cache (model snapshots, refs, blobs)
HUGGINGFACE_HUB_CACHE=models/.cache/huggingface/hub

# Transformers cache (legacy but still used by some codepaths)
TRANSFORMERS_CACHE=models/.cache/huggingface/transformers

# Datasets cache (if later you use datasets for eval or corpora)
HF_DATASETS_CACHE=models/.cache/huggingface/datasets

# Optional: cache for tokenizers (sometimes used separately)
HF_TOKENIZERS_CACHE=models/.cache/huggingface/tokenizers

# Optional: cache for custom modules loaded from HF repos
HF_MODULES_CACHE=models/.cache/huggingface/modules

# Optional: where to store GGUF files if you download them directly
GGUF_CACHE_DIR=models/gguf

# Optional: where to store compiled/optimized artifacts
MODEL_BUILD_CACHE_DIR=models/.cache/build

# Disable progress bars in non-interactive environments
HF_HUB_DISABLE_PROGRESS_BARS=1
TQDM_DISABLE=1

# Hugging Face token (optional unless you pull gated models or need higher rate limits)
HF_TOKEN=
HUGGINGFACE_HUB_TOKEN=

## =================================================================
## Logging
## =================================================================
LOG_DIR=logs
LOG_LEVEL=INFO
LOG_CONSOLE_LEVEL=INFO
LOG_FORMAT=json
LOG_ROTATION=00:00
LOG_RETENTION=30 days
LOG_COMPRESSION=zip
LOG_MASK_AUTH=true
LOG_HTTP_LEVEL=WARNING
LOG_SPLIT_FILES=true
LOG_ENABLE_HTTP_FILE=false